{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: TF-IDF + Cosine Similarity (KNN Preperation)\n",
    "\n",
    "__Created By:__ Sharan Jhangiani\n",
    "__Due:__ May 8th, 2019 by 11:59PM\n",
    "\n",
    "This lab will introduce you to concepts you should know for the upcoming problem set on KNN. Specifially, we'll focus on different KNN similarity measures, how to calculate them, what they mean, and how to do some basic text classification using them.\n",
    "\n",
    "Please follow all instructions carefully, and submit an HTML or PDF copy of this jupyter notebook.\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "We'll create our data set below. It represents 3 different movie reviews from 3 movies. We'll work on finding TF-IDF scores for data. It contains 3 features: review, review_id, and movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data\n",
    "review_text = ['A triumph of sorts. Not of acting: with some notable exceptions — a beer-bellied, tragicomic Chris Hemsworth as Thor among them — a feeling of exhaustion has crept in with a number of the principals over the 10 or so years since Iron Man set this gargantuan, quippy super-opera in motion. Nor of dialogue: not even Don Cheadle can deliver a line like “We are all about that superhero life,” and the little speeches tend to fall flat, to the point where the film gets self-conscious about them. Nor of direction: for the first hour or so, the Russo brothers rely heavily on your good will toward the superfolk who remain in the wake of “blue meanie” Thanos’ halving of the universe’s population, and the relentless stream of fan service in the two hours that follow may leave even the truest believer feeling milked. But a triumph of feeling: of itches scratched, of sorrows and angers eased, even of longings fulfilled. A triumph of theme: the team’s [SPOILER ALERT?] time-travel gambit hammers home the Why We Fight virtues of home and hearth, of Mom and Dad and the kids. And above all, a triumph of plotting: thread after narrative thread tied off in neat and sometimes unexpected (and unexpectedly satisfying) fashion. The MCU will go on and on, but this chapter — and the American pragmatism vs. American ideals bromance that drove it — have well and truly come to their “Excelsior! Nuff said!” moment. 2019.', \n",
    "                 'Going into this film I had questions about Brie Larsons casting but expected a great story that fed off the current state of the MCU. Turns out I had that back asswards. Larson fit the role perfectly and puts on a great performance, but the story is pretty lazy and the humor is disappointingly corny, especially when you consider how surprisingly funny Marvel movies are capable of being. I also hope future movies include a patch that sees a Captain Marvel nerf just to keep things interesting, but at the same time, I enjoy watching action sequences of her overpowering everything in her way. this film is like a cheese platter; the Brie almost saves it, but in the end its just an average appetizer to the main course that is End Game.', \n",
    "                 'It has been eleven years and 21 films until this point and whether or not you have been a fan of each and every film or not, its undeniable that what producer Kevin Feige has been spearheading all these years is unprecedented and will probably be at least another decade until something like this is even attempted again. Even though Avengers: Infinity War felt like every storyline was colliding last year, Avengers: Endgame is the true finale of what they are now calling the Infinity Saga. Beginning with Iron Man in 2008 and finishing with Avengers: Endgame in 2019, this is a franchise that was made for fans of this newfound genre in superhero filmmaking and Avengers: Endgame is a near-perfect conclusion in every way. Picking up shortly after the events of Avengers: Infinity War (and dont worry, I wont be spoiling anything from this movie), we meet the team at their lowest point in the entire franchise. The first act of this film is really just about them coping, but shimmers of hope begin to arise, giving them ideas and possibilities for a new future going forward. The journey this film takes you on is absolutely astounding and there are some moments that will have you thinking the franchise has officially peaked, but I feel that fans have earned those moments for being invested through thing many films. Theres honestly only so much to say without ruining the experience, so Ill move on to some specifics here. Robert Downey Jr. has always been great as Tony Stark, but due to the circumstances left by the previous film, I truly dont think hes ever been as good on-screen as he is in this film. He carries the emotional weight of the film on his shoulders and I just wanted to applaud every second he was on camera. On top of that, everyone from Karen Gillan (Nebula) getting more to do than she has ever done, to Chris Evans truly earning back his characters name, to even Jeremy Renner, who hasnt exactly had many standout moments until now, every cast member brings their all here and I could go on for days about how Avengers: Endgame features the most devoted cast any of these films have seen. I would also like to commend Scarlett Johansson, as it has been years since Ive seen her this committed to a character. Audiences will flock to this movie hoping for a lot of big-budget spectacle, and although there are some of the craziest sequences in the franchise thus far, I have to admit that fans of drama will also probably appreciate this movie more than most of the films in the franchise. The impact the previous film has on this movie is severe and it really shows throughout the first two acts. From some great cinematography and new visuals weve yet to see until now, this is the definition of a film that has it all. Theres so much I would like to dive into here, but Ill just say this is the definition of a movie that people dream up when watching a franchise. Story threads are tied up nicely, the spectacle is dialled up to a million, the performances are probably the best that this franchise has ever seen, and the humour is absolutely still present and well-balanced with the drama in a way that feels needed for this heavy movie. Overall, Avengers: Endgame isnt a perfect movie, because really, no film of this size can truly be perfect, but this was one of the best theatre-going experiences Ive had in years. This is a true finale that earns every second of its three hour run time. Heres to a bright future for the Marvel Cinematic Universe.']\n",
    "\n",
    "review_id = ['Review 1', 'Review 2', 'Review 3']\n",
    "\n",
    "movies = ['Avengers Endgame', 'Captain Marvel', 'Avengers Endgame']\n",
    "\n",
    "d = {'review_text' : review_text, 'review_id' : review_id, 'movies' : movies}\n",
    "\n",
    "data = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>review_id</th>\n",
       "      <th>movies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A triumph of sorts. Not of acting: with some n...</td>\n",
       "      <td>Review 1</td>\n",
       "      <td>Avengers Endgame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Going into this film I had questions about Bri...</td>\n",
       "      <td>Review 2</td>\n",
       "      <td>Captain Marvel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It has been eleven years and 21 films until th...</td>\n",
       "      <td>Review 3</td>\n",
       "      <td>Avengers Endgame</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text review_id  \\\n",
       "0  A triumph of sorts. Not of acting: with some n...  Review 1   \n",
       "1  Going into this film I had questions about Bri...  Review 2   \n",
       "2  It has been eleven years and 21 films until th...  Review 3   \n",
       "\n",
       "             movies  \n",
       "0  Avengers Endgame  \n",
       "1    Captain Marvel  \n",
       "2  Avengers Endgame  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model, we want to be able to compare these lines of code with each other. But since they are text values, our machines are unable to do so unless we can break them down into a _numerical_ format. To do so, we use a metric called __TF-IDF__.\n",
    "\n",
    "TF-IDF broken down is:\n",
    "\n",
    "- TF(Total Frequency): The frequency of the word in a current document over the number of total words in all documents.\n",
    "\n",
    "- IDF(Inverse Document Frequency): The number of documents the word appears in over the number of total documents.\n",
    "\n",
    "Lets start with Total Frequency. We first need to get the count for each word in all documents across the dataset. This is a Bag of Words (BOW). \n",
    "\n",
    "__First combine all the reviews so we have a list of every word said in every review.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_rev = ' '.join(review_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of all our words, we'll convert them into frequency counts. We could do this with a for loop, but for time and efficiency sake, we'll use a method from sklearn called Vectorizer. \n",
    "\n",
    "__Create your vectorizer and find the bag of words for the given documents.__\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 1 ... 0 0 1]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 1 1 ... 1 3 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())  \n",
    "\n",
    "word_array = X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created our bag of words. This represents the frequency of each word in each document. __Take a look at the shape:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 459)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first value represents the number of documents and the second value represents the number of unique words in the reviews we are looking at. __To better understand the shape of the bag of words, convert the numpy array to a dataframe with the feature names as columns. Print out your result.__\n",
    "\n",
    "Hint: you get the feature names from the vectorizer, not the bag of words itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '2008', '2019', '21', 'about', 'above', 'absolutely', 'act', 'acting', 'action', 'acts', 'admit', 'after', 'again', 'alert', 'all', 'almost', 'also', 'although', 'always', 'american', 'among', 'an', 'and', 'angers', 'another', 'any', 'anything', 'appetizer', 'applaud', 'appreciate', 'are', 'arise', 'as', 'asswards', 'astounding', 'at', 'attempted', 'audiences', 'avengers', 'average', 'back', 'balanced', 'be', 'because', 'been', 'beer', 'begin', 'beginning', 'being', 'believer', 'bellied', 'best', 'big', 'blue', 'brie', 'bright', 'brings', 'bromance', 'brothers', 'budget', 'but', 'by', 'calling', 'camera', 'can', 'capable', 'captain', 'carries', 'cast', 'casting', 'chapter', 'character', 'characters', 'cheadle', 'cheese', 'chris', 'cinematic', 'cinematography', 'circumstances', 'colliding', 'come', 'commend', 'committed', 'conclusion', 'conscious', 'consider', 'coping', 'corny', 'could', 'course', 'craziest', 'crept', 'current', 'dad', 'days', 'decade', 'definition', 'deliver', 'devoted', 'dialled', 'dialogue', 'direction', 'disappointingly', 'dive', 'do', 'don', 'done', 'dont', 'downey', 'drama', 'dream', 'drove', 'due', 'each', 'earned', 'earning', 'earns', 'eased', 'eleven', 'emotional', 'end', 'endgame', 'enjoy', 'entire', 'especially', 'evans', 'even', 'events', 'ever', 'every', 'everyone', 'everything', 'exactly', 'excelsior', 'exceptions', 'exhaustion', 'expected', 'experience', 'experiences', 'fall', 'fan', 'fans', 'far', 'fashion', 'features', 'fed', 'feel', 'feeling', 'feels', 'feige', 'felt', 'fight', 'film', 'filmmaking', 'films', 'finale', 'finishing', 'first', 'fit', 'flat', 'flock', 'follow', 'for', 'forward', 'franchise', 'from', 'fulfilled', 'funny', 'future', 'gambit', 'game', 'gargantuan', 'genre', 'gets', 'getting', 'gillan', 'giving', 'go', 'going', 'good', 'great', 'had', 'halving', 'hammers', 'has', 'hasnt', 'have', 'he', 'hearth', 'heavily', 'heavy', 'hemsworth', 'her', 'here', 'heres', 'hes', 'his', 'home', 'honestly', 'hope', 'hoping', 'hour', 'hours', 'how', 'humor', 'humour', 'ideals', 'ideas', 'ill', 'impact', 'in', 'include', 'infinity', 'interesting', 'into', 'invested', 'iron', 'is', 'isnt', 'it', 'itches', 'its', 'ive', 'jeremy', 'johansson', 'journey', 'jr', 'just', 'karen', 'keep', 'kevin', 'kids', 'larson', 'larsons', 'last', 'lazy', 'least', 'leave', 'left', 'life', 'like', 'line', 'little', 'longings', 'lot', 'lowest', 'made', 'main', 'man', 'many', 'marvel', 'may', 'mcu', 'meanie', 'meet', 'member', 'milked', 'million', 'mom', 'moment', 'moments', 'more', 'most', 'motion', 'move', 'movie', 'movies', 'much', 'name', 'narrative', 'near', 'neat', 'nebula', 'needed', 'nerf', 'new', 'newfound', 'nicely', 'no', 'nor', 'not', 'notable', 'now', 'nuff', 'number', 'of', 'off', 'officially', 'on', 'one', 'only', 'opera', 'or', 'out', 'over', 'overall', 'overpowering', 'patch', 'peaked', 'people', 'perfect', 'perfectly', 'performance', 'performances', 'picking', 'platter', 'plotting', 'point', 'population', 'possibilities', 'pragmatism', 'present', 'pretty', 'previous', 'principals', 'probably', 'producer', 'puts', 'questions', 'quippy', 'really', 'relentless', 'rely', 'remain', 'renner', 'robert', 'role', 'ruining', 'run', 'russo', 'saga', 'said', 'same', 'satisfying', 'saves', 'say', 'scarlett', 'scratched', 'screen', 'second', 'see', 'seen', 'sees', 'self', 'sequences', 'service', 'set', 'severe', 'she', 'shimmers', 'shortly', 'shoulders', 'shows', 'since', 'size', 'so', 'some', 'something', 'sometimes', 'sorrows', 'sorts', 'spearheading', 'specifics', 'spectacle', 'speeches', 'spoiler', 'spoiling', 'standout', 'stark', 'state', 'still', 'story', 'storyline', 'stream', 'super', 'superfolk', 'superhero', 'surprisingly', 'takes', 'team', 'tend', 'than', 'thanos', 'that', 'the', 'theatre', 'their', 'them', 'theme', 'there', 'theres', 'these', 'they', 'thing', 'things', 'think', 'thinking', 'this', 'thor', 'those', 'though', 'thread', 'threads', 'three', 'through', 'throughout', 'thus', 'tied', 'time', 'to', 'tony', 'top', 'toward', 'tragicomic', 'travel', 'triumph', 'true', 'truest', 'truly', 'turns', 'two', 'undeniable', 'unexpected', 'unexpectedly', 'universe', 'unprecedented', 'until', 'up', 'virtues', 'visuals', 'vs', 'wake', 'wanted', 'war', 'was', 'watching', 'way', 'we', 'weight', 'well', 'weve', 'what', 'when', 'where', 'whether', 'who', 'why', 'will', 'with', 'without', 'wont', 'worry', 'would', 'year', 'years', 'yet', 'you', 'your']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>2008</th>\n",
       "      <th>2019</th>\n",
       "      <th>21</th>\n",
       "      <th>about</th>\n",
       "      <th>above</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>act</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>...</th>\n",
       "      <th>with</th>\n",
       "      <th>without</th>\n",
       "      <th>wont</th>\n",
       "      <th>worry</th>\n",
       "      <th>would</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 459 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   10  2008  2019  21  about  above  absolutely  act  acting  action  ...   \\\n",
       "0   1     0     1   0      2      1           0    0       1       0  ...    \n",
       "1   0     0     0   0      1      0           0    0       0       1  ...    \n",
       "2   0     1     1   1      2      0           2    1       0       0  ...    \n",
       "\n",
       "   with  without  wont  worry  would  year  years  yet  you  your  \n",
       "0     2        0     0      0      0     0      1    0    0     1  \n",
       "1     0        0     0      0      0     0      0    0    1     0  \n",
       "2     3        1     1      1      2     1      4    1    3     0  \n",
       "\n",
       "[3 rows x 459 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "word_df = pd.DataFrame(word_array)\n",
    "\n",
    "word_df.columns = vectorizer.get_feature_names()[:]\n",
    "\n",
    "word_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our bag of words, lets __pick a word and find the current BOW representation of it.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "Name: good, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df.good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_df.iloc[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This BOW metric can be used in KNN classification. However, we get better results with the TF-IDF optimization of the word. __Find the TF of the given word for each document below. Print out your selected word as well as the value for each document.__\n",
    "\n",
    "_Remember, TF stands for total frequency in a given document, so the equation would be \"Number of Occurences of Word in Document / All Words in Document\"._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word - 'good'\n",
      "--------------------\n",
      "Document 1 TF: 0.004 // word appearance: 1, total words: 238\n",
      "Document 2 TF: 0.0 // word appearance: 0, total words: 124\n",
      "Document 3 TF: 0.002 // word appearance: 1, total words: 608\n"
     ]
    }
   ],
   "source": [
    "print(\"Word - 'good'\")\n",
    "print('--------------------')\n",
    "\n",
    "tf_1 = word_df.good[0]/word_df.iloc[0].sum()\n",
    "print('Document 1 TF: ' + str(round(word_df.good[0]/word_df.iloc[0].sum(), 3)) + ' // word appearance: '\n",
    "      + str(word_df.good[0]) + ', total words: ' + str(word_df.iloc[0].sum()))\n",
    "\n",
    "tf_2 = word_df.good[1]/word_df.iloc[1].sum()\n",
    "print('Document 2 TF: ' + str(round(word_df.good[1]/word_df.iloc[1].sum(), 3))\n",
    "      + ' // word appearance: ' +  str(word_df.good[1]) + ', total words: ' + str(word_df.iloc[1].sum()))\n",
    "\n",
    "tf_3 = word_df.good[2]/word_df.iloc[2].sum()\n",
    "print('Document 3 TF: ' + str(round(word_df.good[2]/word_df.iloc[2].sum(), 3))\n",
    "      + ' // word appearance: ' +  str(word_df.good[2]) + ', total words: ' + str(word_df.iloc[2].sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solve for the idf variant of the equation now. Print out the word and the result.__\n",
    "\n",
    "_Remember, the IDF is the Inverse Document Frequency, which you can find by the equation \"log(Number of Documents / Number of Documents the Given Word Appears In)\"._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf for 'good': 0.4054651081081644\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(\"idf for 'good': \" + str(math.log(len(word_df)/sum(word_df.good>0))))\n",
    "\n",
    "idf = math.log(len(word_df)/sum(word_df.good>0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have both sides of the equation, we can multiply the two together to get the final TF-IDF score for each word. __Do so below and print out your results along with your specified word.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF for 'good' in document 1: 0.0017036349080174974\n",
      "TF-IDF for 'good' in document 2: 0.0\n",
      "TF-IDF for 'good' in document 3: 0.0006668834014936914\n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF for 'good' in document 1: \" + str(tf_1*idf))\n",
    "\n",
    "print(\"TF-IDF for 'good' in document 2: \" + str(tf_2*idf))\n",
    "\n",
    "print(\"TF-IDF for 'good' in document 3: \" + str(tf_3*idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the question why do we even bother with TF-IDF, this quote from http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/ explains it well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_However, the main problem with a term-frequency approach is that it scales up frequent terms and scales down rare terms which are empirically more informative than the high frequency terms. The basic intuition is that a term that occurs frequently in many documents is not a good discriminator, and really makes sense (at least in many experimental tests); the important question here is: why would you, in a classification problem for instance, emphasize a term which is almost present in the entire corpus of your documents ?_\n",
    "\n",
    "_The tf-idf weight comes to solve this problem. What tf-idf gives is how important is a word to a document in a collection, and that’s why tf-idf incorporates local and global parameters, because it takes in consideration not only the isolated term but also the term within the document collection. What tf-idf then does to solve that problem, is to scale down the frequent terms while scaling up the rare terms; a term that occurs 10 times more than another isn’t 10 times more important than it, that’s why tf-idf uses the logarithmic scale to do that._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "As Ott discussed in lecture, we need to be able to find the distance between different data points to find the \"nearest neighbors\" when using the KNN model. One common method of finding the distance is through cosine similarity. \n",
    "\n",
    "__Read this following article (up until the \"Practice Using Scikit-learn (sklearn)\" section) and write a paragraph explaining what Cosine Similarity is, how we calculate it, and the underlying mathematical concept of it.__\n",
    "\n",
    "http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity finds the cosine of the angle between two vectors, or between two documents on the vector space. It measures the orientation of the angles instead of magnitude and because of this can be seen as a comparison between the two documents on a normalized space. It's found by solving the dot product equation for the cos$\\theta$, which will give you the equation to find cosine similarity - cos$\\theta$ = $\\frac{\\vec{A} \\cdot \\vec{B}}{||\\vec{A}|| \\times ||\\vec{B}||}$, and it will find how related two documents are through their angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
